Detect Credit Card Fraud with Machine Learning in R

#1. Importing the Datasets
```{r}
library(ranger)
library(caret)
library(data.table)
library(ggplot2) # data visualization
library(caTools) # for train/test split
```

```{r}
setwd("~/Documents/projects/detect_creditCard")
```

```{r}
data <- read.csv("creditcard.csv")
```

#2. Data Exploration
```{r}
dim(data)
head(data)
tail(data)
```

```{r}
summary(data)
```

# check missing value
```{r}
colSums(is.na(data))
```

```{r}
table(data$Class)
```
#Data visualizstion
```{r}
library(ggplot2)
library(dplyr) # for data manipulation
common_theme <- theme(plot.title = element_text(hjust = 0.5, face = "bold"))
data %>%
  ggplot(aes(x = Time, fill = factor(Class))) + geom_histogram(bins = 100)+
  labs(x = 'Time in seconds since first transaction', y = 'No. of transactions') +
  ggtitle('Distribution of time of transaction by class') +
  facet_grid(Class ~ ., scales = 'free_y') + common_theme
```
#3. Data Manipulation
```{r}
data$Amount = scale(data$Amount)
newData = data[,c(-1)]
newData
```

#4. Data Modeling
training: 80%
test: 20%
```{r}
library(caTools)
set.seed(123)
data_sample = sample.split(newData$Class,SplitRatio=0.80)
train = subset(newData,data_sample==TRUE)
test = subset(newData,data_sample==FALSE)
dim(train)
dim(test)
```
```{r}
table(train$Class)
```

#5. Model: Logistic Regression Model
```{r}
Logistic_Model=glm(Class~.,test_data,family=binomial())
summary(Logistic_Model)
```
```{r}
plot(Logistic_Model)
```
In order to assess the performance of our model, we will delineate the ROC curve. ROC is also known as Receiver Optimistic Characteristics. For this, we will first import the ROC package and then plot our ROC curve to analyze its performance.
```{r}
library(pROC)
lr.predict <- predict(Logistic_Model,test_data, probability = TRUE)
auc.gbm = roc(test_data$Class, lr.predict, plot = TRUE, col = "blue")
```
#6. Decision Tree Model
Decision Trees to plot the outcomes of a decision. These outcomes are basically a consequence through which we can conclude as to what class the object belongs to. We will now implement our decision tree model and will plot it using the rpart.plot() function. We will specifically use the recursive parting to plot the decision tree.
```{r}
library(rpart)
library(rpart.plot)
decisionTree_model <- rpart(Class ~ . , data, method = 'class')
predicted_val <- predict(decisionTree_model, data, type = 'class')
probability <- predict(decisionTree_model, data, type = 'prob')
rpart.plot(decisionTree_model)
```

#7. Artificial Neural Network
The ANN models are able to learn the patterns using the historical data and are able to perform classification on the input data. We import the neuralnet package that would allow us to implement our ANNs. Then we proceeded to plot it using the plot() function. Now, in the case of Artificial Neural Networks, there is a range of values that is between 1 and 0. We set a threshold as 0.5, that is, values above 0.5 will correspond to 1 and the rest will be 0. We implement this as follows
```{r}
library(neuralnet)
ANN_model =neuralnet (Class~.,train_data,linear.output=FALSE)
plot(ANN_model)

predANN=compute(ANN_model,test_data)
resultANN=predANN$net.result
resultANN=ifelse(resultANN>0.5,1,0)
```
#8. 8. Gradient Boosting (GBM)
Gradient Boosting is a popular machine learning algorithm that is used to perform classification and regression tasks. This model comprises of several underlying ensemble models like weak decision trees. These decision trees combine together to form a strong model of gradient boosting
```{r}
library(gbm, quietly=TRUE)

# Get the time to train the GBM model
system.time(
       model_gbm <- gbm(Class ~ .
               , distribution = "bernoulli"
               , data = rbind(train_data, test_data)
               , n.trees = 500
               , interaction.depth = 3
               , n.minobsinnode = 100
               , shrinkage = 0.01
               , bag.fraction = 0.5
               , train.fraction = nrow(train_data) / (nrow(train_data) + nrow(test_data))
)
)
# Determine best iteration based on test data
gbm.iter = gbm.perf(model_gbm, method = "test")
```

```{r}
model.influence = relative.influence(model_gbm, n.trees = gbm.iter, sort. = TRUE)
#Plot the gbm model

plot(model_gbm)
```

```{r}
# Plot and calculate AUC on test data
gbm_test = predict(model_gbm, newdata = test_data, n.trees = gbm.iter)
gbm_auc = roc(test_data$Class, gbm_test, plot = TRUE, col = "red")
```

```{r}
print(gbm_auc)
```

```{r}
train_data
```



